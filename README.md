# Awesome-LLM-Alignment [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

## Overview

## LLM for Safety & Safe-Alignment
- **Constrained Value-Aligned LLM via Safe RLHF** \
*Tao Dai, Xuehai Pan, Jiaming Ji, Ruiyang Sun, Yizhou Wang, Yaodong Yang* \
Peking University, Github repo. [[Github](https://github.com/PKU-Alignment/safe-rlhf)] [[HuggingFace](https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K)] \
21 Feb 2023

## LLM for Application

## LLM for Human & Value-Alignment

- **Training Socially Aligned Language Models in Simulated Human Society** \
*Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, Soroush Vosoughi* \
Dartmouth College, arXiv 2023. [[Paper](https://arxiv.org/abs/2305.16960)] [[Github](https://github.com/agi-templar/Stable-Alignment)] [[HuggingFace](https://huggingface.co/papers/2305.16960)] \
21 Feb 2023
