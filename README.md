# Awesome-LLM-Alignment [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

## Overview

## LLM for Safety & Safe-Alignment
- **Constrained Value-Aligned LLM via Safe RLHF** \
*Tao Dai, Xuehai Pan, Jiaming Ji, Ruiyang Sun, Yizhou Wang, Yaodong Yang* \
Peking University, Github repo. [[Github](https://github.com/PKU-Alignment/safe-rlhf)] [[HuggingFace](https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K)] \
21 Feb 2023

## LLM for Application

## LLM for Human & Value-Alignment

---
**Training Socially Aligned Language Models in Simulated Human Society** \
*Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, Soroush Vosoughi*\
Dartmouth College, arXiv 2023. [[Paper](https://arxiv.org/abs/2305.16960)] [[Github](https://github.com/agi-templar/Stable-Alignment)] [[HuggingFace](https://huggingface.co/papers/2305.16960)] \
26 May 2023
<details>
<summary><b>Abstract</b></summary>
Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.
</details>

---
