# Awesome-LLM-Alignment [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

## Overview

## LLM for Safety & Safe-Alignment
- **Constrained Value-Aligned LLM via Safe RLHF** \
*Tao Dai, Xuehai Pan, Jiaming Ji, Ruiyang Sun, Yizhou Wang, Yaodong Yang* \
Peking University, Github repo. [[Github](https://github.com/PKU-Alignment/safe-rlhf)] [[HuggingFace](https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K)] \
21 Feb 2023

### LLM for Evaluation
---
**Can Large Language Models Be an Alternative to Human Evaluations?** \
*Cheng-Han Chiang, Hung-yi Lee*\
National Taiwan University, arXiv 2023. [[Paper](https://arxiv.org/abs/2305.01937)] \
3 May 2023
<details>
<summary><b>Abstract</b></summary>
Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.
</details>

---
## LLM for Application

## LLM for Human & Value-Alignment

---
**Training Socially Aligned Language Models in Simulated Human Society** \
*Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, Soroush Vosoughi*\
Dartmouth College, arXiv 2023. [[Paper](https://arxiv.org/abs/2305.16960)] [[Github](https://github.com/agi-templar/Stable-Alignment)] [[HuggingFace](https://huggingface.co/papers/2305.16960)] \
26 May 2023
<details>
<summary><b>Abstract</b></summary>
Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.
</details>

---
